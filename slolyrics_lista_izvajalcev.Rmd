---
title: 'Scraping song lyrics from Besedilo.si (List of artists)'
author: "Teodor Petriƒç"
date: "2022-09-17 (update: 'r Sys.Date()')"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The workflow of this tutorial has been adapted from [*Niekler & Wiedemann*](https://github.com/tm4ss/tm4ss.github.io)'s tutorial on [scraping newspaper articles](https://github.com/tm4ss/tm4ss.github.io/blob/master/Tutorial_1_Web_scraping.Rmd) and modified to scrape the lyrics of selected songs or albums from the [Genius](https://genius.com/) portal. Our goal is to use the resulting collection of song lyrics for linguistic study. 

This tutorial covers how to extract and process text data from web pages or other documents for later analysis.

The automated download of HTML pages is called **Crawling**. The extraction of the textual data and/or metadata (for example, article date, headlines, author names, article text) from the HTML source code (or the DOM document object model of the website) is called **Scraping**. For these tasks, we use the package `rvest`.

1. Download a single web page and extract its content
2. Extract links from a overview page and extract song lyrics

## Preparation

```{r}
library(tidyverse)
require("rvest")
```

First, make sure your working directory is the data directory we provided for the exercises.

```{r, message=FALSE, eval=F}
# important option for text analysis
options(stringsAsFactors = F)
getwd()
```


## Crawl single webpage

In a first exercise, we will download a single web page from "Der Spiegel" and extract text together with relevant metadata such as the article date. Let's define the URL of the article of interest and load the *rvest* package, which provides very useful functions for web crawling and scraping.

```{r}
url <- "http://slolyrics.com/agro-pop/ti-si-moj-soncek/"
```

A convenient method to download and parse a webpage provides the function `read_html` which accepts a URL as a parameter. The function downloads the page and interprets the html source code as an HTML / XML object. 

```{r}
html_document <- read_html(url)
```

HTML / XML objects are a structured representation of HTML / XML source code, which allows to extract single elements (headlines e.g. `<h1>`, paragraphs `<p>`, links `<a>`, ...), their attributes (e.g. `<a href="http://...">`) or text wrapped in between elements (e.g. `<p>my text...</p>`). Elements can be extracted in XML objects with XPATH-expressions. 

XPATH (see https://en.wikipedia.org/wiki/XPath) is a query language to select elements in XML-tree structures. 
We use it to select the headline element from the HTML page.
The following xpath expression queries for first-order-headline elements `h1`, anywhere in the tree `//` which fulfill a certain condition `[...]`, namely that the `class` attribute of the `h1` element must contain the value `content__headline`.

The next expression uses R pipe operator %>%, which takes the input from the left side of the expression and passes it on to the function ion the right side as its first argument. The result of this function is either passed onto the next function, again via %>% or it is assigned to the variable, if it is the last operation in the pipe chain. Our pipe takes the `html_document` object, passes it to the html_node function, which extracts the first node fitting the given xpath expression. The resulting node object is passed to the `html_text` function which extracts the text wrapped in the `h1`-element.

Instead of XPATH-expressions used in *Niekler & Wiedemann*'s tutorial, we will be using css style expressions (easier to read and write).

Name of artist (izvajalec):

```{r}
# div id = "a" thus input css #a
artist_xpath <- "#a h2"
# in this case not necessary
artist_xpath <- "h2"
artist_text <- html_document %>%
  html_node(artist_xpath) %>%
  html_text(trim = T)

cat(artist_text)

```

Let's see, what the title_text contains:

```{r}
# this works
title_xpath <- "p a"
# but class p is enough
title_xpath <- "p"
title_text <- html_document %>%
  html_node(title_xpath) %>%
  html_text(trim = T)

cat(title_text)

```

Grabbing the link: 

```{r}
# Grabbing the link
title_xpath <- "p a"
title_link <- html_document %>%
  html_node(title_xpath) %>%
  html_attr("href")
title_link

```


The writer (avtor besedila) and the composer (avtor glasbe). 

```{r}
# use id + class
writer_xpath <- "#b h1"
# or just the h1 class
writer_xpath <- "h1"
writer_text <- html_document %>%
  html_node(writer_xpath) %>%
  html_text(trim = T)

cat(writer_text)

```

Who is the performer?

```{r}
performer_xpath <- "#bb p"
performer_text <- html_document %>%
  html_node(performer_xpath) %>%
  html_text(trim = T)

cat(performer_text)

```

Now we modify the xpath expressions, to extract the article info, the paragraphs of the body text and the article date. Note that there are multiple paragraphs in the article. To extract not only the first, but all paragraphs we utilize the `html_nodes` function and glue the resulting single text vectors of each paragraph together with the `paste0` function.

```{r}
# div id = bb
body_xpath <- "#bb div+ p"
body_text <- html_document %>%
  html_nodes(body_xpath) %>%
  # html_text(trim = T) %>% 
  html_text2() %>%
  paste(collapse = "\n")

cat(body_text)

```

```{r eval=T, echo=F}
cat(substr(body_text, 0, 150))
```


```{r}
newtitles_xpath <- "#a a+ a"
newtitles_xpath <- "p:nth-child(8)"
newtitles_text <- html_document %>%
  html_node(newtitles_xpath) %>%
  # html_text(trim = T) %>% 
  # html_text2() %>%
  paste(collapse = "\n") %>%
  # html_attr("href")
  str_squish()

cat(newtitles_text)

```



```{r}
current_song <- as.data.frame(cbind(
    artist_text, writer_text, title_text, body_text))
current_song

library(writexl)
write_xlsx(current_song, 
           paste0("data/slolyrics_",artist_text,"_",title_text,".xlsx"))

```


The variables `title_text`, `intro_text`, `body_text` and `date_object` now contain the raw data for any subsequent text processing.


## Lists of artists' initials

```{r, message=FALSE, warning=FALSE}
options(stringsAsFactors = F)
library(tidyverse)
require(rvest)
```

Other websites (found with keywords "besedila pesmi": 
- http://slolyrics.com/izvajalci/   
- https://www.lyric.si/ (> 2000 lyrics)


Here we scrape lyrics from the website besedilo.si.    
*Lista izvajalcev*: https://www.besedilo.si/lista-izvajalcev

```{r}
list_url <- paste0("http://slolyrics.com/", "izvajalci/")

html_document <- read_html(list_url)
html_document
```

Collect the links on the starting page to find all artists. 

```{r}
# div id = "b"
links_xpath <- "#b h1+ p a"
# or just
links_xpath <- "h1+ p a"
links <- html_document %>%
  html_nodes(links_xpath) %>%
  html_attr(name = "href")

# the full web addresses
links <- paste0("http://slolyrics.com/", links)
links <- links[-1]
# 26 pages with artist names
links
```

## Collect links of artist names

We open the first page filled with links to artist names beginning with the letter A. 

```{r}
base_url <- "http://slolyrics.com/"

# links[1] ist the page with letters

artists_links_list <- list()

tictoc::tic()
for(i in 1:length(links)){
  # read page with artist name beginning with a
  html_document <- read_html(links[i])
  
  artists_xpath <- "#b p a"
  # more accurate
  artists_xpath <- "h1+ p a"
  artists_links <- html_document %>%
    html_nodes(artists_xpath) %>%
    html_attr(name = "href")
  
  # the full web addresses
  artists_links <- paste0(base_url, artists_links)
  artists_links_list <- append(artists_links_list, artists_links)
  artists_links_list <- unlist(artists_links_list)
}

# 26 pages with artist names
head(artists_links_list)
tail(artists_links_list)
length(artists_links_list)
tictoc::toc()

```

## How many views? 

This took me 1195.69 sec for 59435 rows of song titles on my laptop. 

```{r}
all_views <- NULL

tictoc::tic()
for(i in 1:length(artists_links_list)){
artist_page <- read_html(artists_links_list[i])
artist <- str_split(artists_links_list[i], "/") %>% 
  unlist() %>% .[4] %>% str_replace("-", " ")

# ogledi_xpath <- "#b p #text"
ogledi_xpath <- "p:nth-child(3)"
ogledi_text <- artist_page %>%
  html_node(ogledi_xpath) %>%
  html_text2() %>% 
  str_squish()

ogledi_text <- ogledi_text %>% 
  tolower() %>% 
  str_split(pattern = "ogledov") %>% 
  unlist() %>% 
  str_squish()

ogledi_text <- ogledi_text %>% 
  as_tibble() %>% 
  filter(value != "") %>% 
  # separate column from the end !!! (sep = -4)
  separate(value, into = c('value', 'views'), 
           sep = -4, convert = TRUE) %>% 
  mutate(views = str_remove(views, "- ")) %>% 
  mutate(views = parse_number(views)) %>% 
  mutate(value = str_remove(value, "- $")) %>% 
  mutate(value = str_remove(value, "-$")) %>% 
  mutate(value = str_replace(value, " - ", "_")) %>% 
  separate(value, 
           into = c("artist", "title"), sep = "_", 
           extra = "merge", fill = "right")

all_views <- bind_rows(all_views, ogledi_text)
  
}

tictoc::toc()

```

Quite some song titles are duplicated or trippled because different users have uploaded their separate versions. 

```{r}
# today <- lubridate::today()
# saveRDS(all_views, paste0("data/slolyrics_views",today,".rds"))
```

```{r}
# all_views <- read_rds("data/slolyrics_views2022-10-04.rds")
```


## Collect song links

Now, we have got all artists and we have to fetch the pages with the lyrics. This takes about 1240.65 seconds (more than 20 minutes) on my laptop. We have collected 59436 song links.  

```{r}
songs_links_list <- list()

tictoc::tic()
for(i in 1:length(artists_links_list)){
  # read page with artist name beginning with a
  html_document <- read_html(artists_links_list[i])

  # song_xpath <- "#b p a"
  # more accurate
  song_xpath <- "#b a+ a"
  song_links <- html_document %>%
    html_nodes(song_xpath) %>%
    html_attr(name = "href")
  
    # the full web addresses
  song_links <- paste0(base_url, song_links)
  songs_links_list <- c(songs_links_list, song_links)

}

songs_links_list <- unlist(songs_links_list)
tictoc::toc()

head(songs_links_list)
tail(songs_links_list)
length(songs_links_list)
```

```{r}
# saveRDS(songs_links_list, "data/slolyrics_songs_links_list.rds")
# saveRDS(songs_links_list, "data/slolyrics_songs_links_list.RData")
```

```{r}
# songs_links_list <- 
#   read_rds("data/slolyrics_songs_links_list.rds")
```

Now, `songs_links_list` contains a list of `r length(songs_links_list)` hyperlinks to single song lyrics. 


## Scrape function

An effective way of programming is to encapsulate repeatedly used code in a specific function. This function then can be called with specific parameters, process something and return a result. We use this here, to encapsulate the downloading and parsing of a Guardian article given a specific URL. The code is the same as in our exercise 1 above, only that we combine the extracted texts and metadata in a data.frame and wrap the entire process in a function-Block.

```{r}
artist_xpath <- "h2" # artist name on the left pane
title_xpath <- "p a" # first title on the left pane
writer_xpath <- "h1" # artist + title
body_xpath <- "#bb div+ p"
# youtube_xpath <- "div.video-container iframe"

url <- "http://slolyrics.com/a/6-oclock/"
scrape_slolyrics <- function(url) {
  
  html_document <- read_html(url)
  
  artist_text <- html_document %>%
    html_node(artist_xpath) %>%
    html_text(trim = T)
  
  writer_text <- html_document %>%
    html_node(writer_xpath) %>%
    html_text(trim = T)
  
  title_text <- html_document %>%
    html_node(title_xpath) %>%
    html_text(trim = T)
  
  body_text <- html_document %>%
    html_nodes(body_xpath) %>%
    # html_text(trim = T) %>%
    html_text2() %>% # preserves verse form
    paste0(collapse = "\n")
  
  # youtube_text <- html_document %>%
  #   html_node(youtube_xpath)
  # youtube_text <- str_extract_all(as.character(youtube_text), "(https://www.youtube.com/embed/[a-zA-Z_\\-\\d]+)") %>% unlist()
  
  artist_lyrics <- data.frame(
    artist = artist_text,
    writer = writer_text,
    title = title_text,
    text = body_text,
    # youtube = youtube_text,
    url = url 
    
  )
  
  return(artist_lyrics)
  
}

```


## Reset dataframe

```{r}
all_lyrics <- NULL
all_lyrics <- data.frame()

```


## Download pages

Now we can use that function `scrape_slolyrics` in any other part of our script. For instance, we can loop over each of our collected links. We use a running variable i, taking values from 1 to `length(songs_links_list)` to access the single links in `songs_links_list` and write some progress output.

Last song:  
The second download (from song 22544 to 59436) took my laptop 13297.55 seconds.

```{r}
tictoc::tic()
library(RCurl)
for (i in 1:length(songs_links_list)){
  # skip non-existing html pages
  if (url.exists(songs_links_list[i])) {
    # download from collected links
    cat("Downloading", i, "of", 
        length(songs_links_list), "URL:", songs_links_list[i], "\n")
    lyrics <- scrape_slolyrics(songs_links_list[i])
    # Append current song to the data.frame of all song lyrics
    all_lyrics <- rbind(all_lyrics, lyrics)
    Sys.sleep(0.1)
  } 
  else {
    next}
}

tictoc::toc()
```

## Table

```{r eval=FALSE}
all_lyrics <- all_lyrics %>% 
  mutate(writer = case_when(
    writer == "" | is.na(writer) ~ writer_name, 
    TRUE ~ writer)) %>% 
  mutate(artist = case_when(
    artist == "" | is.na(artist) ~ artist_name, 
    TRUE ~ artist))
all_lyrics
```

## Occasional filtering

```{r eval=FALSE}
# all_lyrics <- all_lyrics %>%
# filter(str_detect(title, "Zeit"))

# all_lyrics <- all_lyrics %>%
#   mutate(released = "Released September 30, 2006") %>%
#   unite(metadata2, c(released, metadata), sep = "\n") %>%
#   rename(metadata = metadata2)

all_lyrics <- all_lyrics %>%
  mutate(youtube = str_replace(youtube, "embed/", "watch?v="))
all_lyrics <- all_lyrics %>%
  filter(text != "")
```


## Save all

```{r}
today = lubridate::today()

saveRDS(all_lyrics, 
           paste0("data/slolyrics_",today,".rds"))
write.csv(all_lyrics, 
           paste0("data/slolyrics_",today,".csv"))
write.csv2(all_lyrics, 
           paste0("data/slolyrics_",today,
                  "_utf8",".csv"), fileEncoding = "UTF-8")
library(writexl)
write_xlsx(all_lyrics, 
           paste0("data/slolyrics_",today,".xlsx"))
```

## Read and join

```{r eval=FALSE}
one <- read_rds("data/slolyrics_2022-10-05.rds")
two <- read_rds("data/slolyrics_2022-10-06.rds")
slolyrics <- bind_rows(one, two)
```

```{r eval=FALSE}
slolyrics_joined <- slolyrics %>% 
  select(-title) %>% 
  mutate(writer = str_replace(writer, " - ", "_")) %>% 
  separate(writer, into = c("izvajalec", "title"), sep = "_",
           extra = "merge") %>% 
  select(-izvajalec)
```

```{r eval=FALSE}
slolyrics_joined %>% 
  count(artist, sort = T) %>% rmarkdown::paged_table()
```

## Save joined

```{r eval=FALSE}
today = lubridate::today()

saveRDS(slolyrics_joined, 
           paste0("data/slolyrics_joined_",today,".rds"))
write.csv(slolyrics_joined, 
           paste0("data/slolyrics_joined_",today,".csv"))
write.csv2(slolyrics_joined, 
           paste0("data/slolyrics_joined_",today,
                  "_utf8",".csv"), fileEncoding = "UTF-8")
library(writexl)
write_xlsx(slolyrics_joined, 
           paste0("data/slolyrics_joined_",today,".xlsx"))
```

